{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import config\n",
    "from MNIST_model import _input_fn, streaming_parser, model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a petastorm parquet file with MNIST data\n",
    "\n",
    "Code from https://github.com/uber/petastorm/blob/master/examples/mnist/generate_petastorm_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from examples.mnist import DEFAULT_MNIST_DATA_PATH\n",
    "from examples.mnist.schema import MnistSchema\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row\n",
    "\n",
    "\n",
    "def download_mnist_data(download_dir, train=True):\n",
    "    \"\"\"\n",
    "    Downloads the dataset files and returns the torch Dataset object, which\n",
    "    represents the data as an array of (img, label) pairs.\n",
    "    Each image is a PIL.Image of black-and-white 28x28 pixels.\n",
    "    Each label is a long integer representing the digit 0..9.\n",
    "    \"\"\"\n",
    "    # This is the only function requiring torch in this module.\n",
    "\n",
    "    # Must import pyarrow before torch. See: https://github.com/uber/petastorm/blob/master/docs/troubleshoot.rst\n",
    "    import pyarrow  # noqa: F401 pylint: disable=W0611,W0612\n",
    "    from torchvision import datasets\n",
    "    print(\"In download_mnist_data...\")\n",
    "    return datasets.MNIST('{}/{}'.format(download_dir, 'data'), train=train, download=True)\n",
    "\n",
    "\n",
    "def mnist_data_to_petastorm_dataset(download_dir, output_url, spark_master=None, parquet_files_count=1,\n",
    "                                    mnist_data=None):\n",
    "    \"\"\"Converts a directory with MNIST data into a petastorm dataset.\n",
    "    Data files are as specified in http://yann.lecun.com/exdb/mnist/:\n",
    "        * train-images-idx3-ubyte.gz:  training set images (9912422 bytes)\n",
    "        * train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)\n",
    "        * t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)\n",
    "        * t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n",
    "    The images and labels and stored in the IDX file format for vectors and multidimensional matrices of\n",
    "    various numerical types, as defined in the same URL.\n",
    "    :param download_dir: the path to where the MNIST data will be downloaded.\n",
    "    :param output_url: the location where your dataset will be written to. Should be a url: either\n",
    "      file://... or hdfs://...\n",
    "    :param spark_master: A master parameter used by spark session builder. Use default value (None) to use system\n",
    "      environment configured spark cluster. Use 'local[*]' to run on a local box.\n",
    "    :param mnist_data: A dictionary of MNIST data, with name of dataset as key, and the dataset object as value;\n",
    "      if None is suplied, download it.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print(f\"Local download dir is {download_dir}\")\n",
    "    session_builder = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('MNIST Dataset Creation')\n",
    "    if spark_master:\n",
    "        session_builder.master(spark_master)\n",
    "\n",
    "    spark = session_builder.getOrCreate()\n",
    "\n",
    "    # Get training and test data\n",
    "    if mnist_data is None:\n",
    "        mnist_data = {\n",
    "            'train': download_mnist_data(download_dir, train=True),\n",
    "            'test': download_mnist_data(download_dir, train=False)\n",
    "        }\n",
    "\n",
    "    print(f\"Done with local download. Starting parquet generation.\")\n",
    "    # The MNIST data is small enough to do everything here in Python\n",
    "    for dset, data in mnist_data.items():\n",
    "        dset_output_url = '{}/{}'.format(output_url, dset)\n",
    "        # Using row_group_size_mb=1 to avoid having just a single rowgroup in this example. In a real store, the value\n",
    "        # should be similar to an HDFS block size.\n",
    "        with materialize_dataset(spark, dset_output_url, MnistSchema, row_group_size_mb=1):\n",
    "            # List of [(idx, image, digit), ...]\n",
    "            # where image is shaped as a 28x28 numpy matrix\n",
    "            idx_image_digit_list = map(lambda idx_image_digit: {\n",
    "                MnistSchema.idx.name: idx_image_digit[0],\n",
    "                MnistSchema.digit.name: idx_image_digit[1][1],\n",
    "                MnistSchema.image.name: np.array(list(idx_image_digit[1][0].getdata()), dtype=np.uint8).reshape(28, 28)\n",
    "            }, enumerate(data))\n",
    "\n",
    "            # Convert to pyspark.sql.Row\n",
    "            sql_rows = map(lambda r: dict_to_spark_row(MnistSchema, r), idx_image_digit_list)\n",
    "\n",
    "            # Write out the result\n",
    "            spark.createDataFrame(sql_rows, MnistSchema.as_spark_schema()) \\\n",
    "                .coalesce(parquet_files_count) \\\n",
    "                .write \\\n",
    "                .option('compression', 'none') \\\n",
    "                .parquet(dset_output_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local download dir is /tmp/mnist\n",
      "In download_mnist_data...\n",
      "In download_mnist_data...\n",
      "Done with local download. Starting parquet generation.\n"
     ]
    }
   ],
   "source": [
    "download_dir = DEFAULT_MNIST_DATA_PATH\n",
    "mnist_data_to_petastorm_dataset(download_dir, config.DATASET_URL)\n",
    "if os.path.exists(download_dir):\n",
    "    shutil.rmtree(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify input by showing an MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy args object so code can be directly copied from MNIST_model.py and checked here\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.dataset_url = config.DATASET_URL\n",
    "args.prefetch_size = 2\n",
    "args.batch_size = 10\n",
    "args.num_parallel_batches = 2\n",
    "worker_index=0\n",
    "nr_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 21:37:44.089046 139620851508992 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0626 21:37:44.089930 139620851508992 deprecation.py:323] From /mnt/tf_example.py:95: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W0626 21:37:44.090412 139620851508992 deprecation.py:323] From /opt/miniconda/envs/py3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "with make_reader(os.path.join(args.dataset_url, 'train'),\n",
    "                 num_epochs=None,\n",
    "                 workers_count=nr_workers,\n",
    "                 shard_count=nr_workers,\n",
    "                 cur_shard=worker_index,) as reader:\n",
    "    exp_dataset = _input_fn(reader = reader,\n",
    "                            batch_size=args.batch_size,\n",
    "                            num_parallel_batches=args.num_parallel_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = exp_dataset.make_one_shot_iterator().get_next()\n",
    "with tf.Session() as sess:\n",
    "    features_manifested, labels_manifested = sess.run([features, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of features should be (10, 784) with 784 = 28 x 28 unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_manifested['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_manifested.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efb618c5f60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "index = random.randint(0, 9)\n",
    "\n",
    "print(labels_manifested[index])\n",
    "plt.imshow(features_manifested['image'][index,:].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with batches and shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, contains 1000 records, total records read is 1000\n",
      "Batch 1, contains 1000 records, total records read is 2000\n",
      "Batch 2, contains 1000 records, total records read is 3000\n",
      "Batch 3, contains 1000 records, total records read is 4000\n",
      "Batch 4, contains 1000 records, total records read is 5000\n",
      "Batch 5, contains 1000 records, total records read is 6000\n",
      "Batch 6, contains 1000 records, total records read is 7000\n",
      "Batch 7, contains 1000 records, total records read is 8000\n",
      "Batch 8, contains 1000 records, total records read is 9000\n",
      "Batch 9, contains 1000 records, total records read is 10000\n",
      "Batch 10, contains 1000 records, total records read is 11000\n",
      "Batch 11, contains 1000 records, total records read is 12000\n",
      "Batch 12, contains 1000 records, total records read is 13000\n",
      "Batch 13, contains 1000 records, total records read is 14000\n",
      "Batch 14, contains 1000 records, total records read is 15000\n",
      "Batch 15, contains 1000 records, total records read is 16000\n",
      "Batch 16, contains 1000 records, total records read is 17000\n",
      "Batch 17, contains 1000 records, total records read is 18000\n",
      "Batch 18, contains 1000 records, total records read is 19000\n",
      "Batch 19, contains 1000 records, total records read is 20000\n",
      "Batch 20, contains 1000 records, total records read is 21000\n",
      "Batch 21, contains 1000 records, total records read is 22000\n",
      "Batch 22, contains 1000 records, total records read is 23000\n",
      "Batch 23, contains 1000 records, total records read is 24000\n",
      "Batch 24, contains 1000 records, total records read is 25000\n",
      "Batch 25, contains 1000 records, total records read is 26000\n",
      "Batch 26, contains 1000 records, total records read is 27000\n",
      "Batch 27, contains 1000 records, total records read is 28000\n",
      "Batch 28, contains 1000 records, total records read is 29000\n",
      "Batch 29, contains 1000 records, total records read is 30000\n",
      "Batch 30, contains 1000 records, total records read is 31000\n",
      "Batch 31, contains 1000 records, total records read is 32000\n",
      "Batch 32, contains 1000 records, total records read is 33000\n",
      "Batch 33, contains 1000 records, total records read is 34000\n",
      "Batch 34, contains 1000 records, total records read is 35000\n",
      "Batch 35, contains 1000 records, total records read is 36000\n",
      "Batch 36, contains 1000 records, total records read is 37000\n",
      "Batch 37, contains 1000 records, total records read is 38000\n",
      "Batch 38, contains 1000 records, total records read is 39000\n",
      "Batch 39, contains 1000 records, total records read is 40000\n",
      "Batch 40, contains 1000 records, total records read is 41000\n",
      "Batch 41, contains 1000 records, total records read is 42000\n",
      "Batch 42, contains 1000 records, total records read is 43000\n",
      "Batch 43, contains 1000 records, total records read is 44000\n",
      "Batch 44, contains 1000 records, total records read is 45000\n",
      "Batch 45, contains 1000 records, total records read is 46000\n",
      "Batch 46, contains 1000 records, total records read is 47000\n",
      "Batch 47, contains 1000 records, total records read is 48000\n",
      "Batch 48, contains 1000 records, total records read is 49000\n",
      "Batch 49, contains 1000 records, total records read is 50000\n",
      "Batch 50, contains 1000 records, total records read is 51000\n",
      "Batch 51, contains 1000 records, total records read is 52000\n",
      "Batch 52, contains 1000 records, total records read is 53000\n",
      "Batch 53, contains 1000 records, total records read is 54000\n",
      "Batch 54, contains 1000 records, total records read is 55000\n",
      "Batch 55, contains 1000 records, total records read is 56000\n",
      "Batch 56, contains 1000 records, total records read is 57000\n",
      "Batch 57, contains 1000 records, total records read is 58000\n",
      "Batch 58, contains 1000 records, total records read is 59000\n",
      "Batch 59, contains 1000 records, total records read is 60000\n",
      "Batch 60, contains 1000 records, total records read is 61000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from petastorm import make_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "\n",
    "def streaming_parser(serialized_example):   \n",
    "    image_data = tf.cast(tf.reshape(serialized_example.image, [784]), tf.float32)\n",
    "    label = serialized_example.digit\n",
    "    return {\"image_data\": image_data}, label\n",
    "\n",
    "\n",
    "# Watch out, cur_shard is 0-based, so a value of 2 (shard_count) will raise OutOfRangeError\n",
    "# This error will be silenced in Tensorflow Estimator 😱\n",
    "with make_reader(os.path.join(args.dataset_url, 'train'),\n",
    "                 num_epochs=None,\n",
    "                 workers_count=2,\n",
    "                 shard_count=2,\n",
    "                 cur_shard=1,) as reader:\n",
    "    exp_dataset = _input_fn(reader = reader,\n",
    "                            batch_size=1000,\n",
    "                            num_parallel_batches=2)\n",
    "\n",
    "    features, labels = exp_dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cum_count = 0\n",
    "        for idx in range(61):  # see what happens if we reach the end of the data\n",
    "            labels_manifested = sess.run([labels])\n",
    "            count = labels_manifested[0].shape[0]\n",
    "            cum_count += count\n",
    "            print(f\"Batch {idx}, contains {count} records, total records read is {cum_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
